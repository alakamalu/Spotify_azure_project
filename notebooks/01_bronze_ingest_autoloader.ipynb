"{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"e982d5fe\",\n   \"metadata\": {},\n   \"source\": [\n    \"# 01 \u2014 Bronze Layer: Ingestion with Autoloader\\n\",\n    \"This notebook shows a Databricks-style Autoloader streaming ingestion example. Replace the `storage_account` variables to run in your environment.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"6f606ddf\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Parameters - set your storage and environment variables here\\n\",\n    \"storage_account = \\\"datalakeproject\\\"\\n\",\n    \"container_bronze = \\\"bronze\\\"\\n\",\n    \"container_silver = \\\"silver\\\"\\n\",\n    \"container_gold = \\\"gold\\\"\\n\",\n    \"base_bronze = f\\\"abfss://{container_bronze}@{storage_account}.dfs.core.windows.net\\\"\\n\",\n    \"base_silver = f\\\"abfss://{container_silver}@{storage_account}.dfs.core.windows.net\\\"\\n\",\n    \"base_gold = f\\\"abfss://{container_gold}@{storage_account}.dfs.core.windows.net\\\"\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"39cc1d7f\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Example Autoloader stream (PySpark / Databricks)\\n\",\n    \"bronze_path = f\\\"{base_bronze}/dim_user\\\"\\n\",\n    \"schema_location = f\\\"{base_silver}/dim_user/schema\\\"\\n\",\n    \"checkpoint_location = f\\\"{base_silver}/dim_user/checkpoint\\\"\\n\",\n    \"print('bronze_path =', bronze_path)\\n\",\n    \"print('schema_location =', schema_location)\\n\",\n    \"print('checkpoint_location =', checkpoint_location)\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"c5cb1aa0\",\n   \"metadata\": {},\n   \"source\": [\n    \"**Autoloader example (concept):**\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"93e5c2ed\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# NOTE: The following code is Databricks PySpark streaming pseudocode.\\n\",\n    \"# In Databricks, run inside a cluster. This will not execute in plain Jupyter without Spark.\\n\",\n    \"df_user = (\\n\",\n    \"    spark.readStream\\n\",\n    \"         .format('cloudFiles')\\n\",\n    \"         .option('cloudFiles.format', 'parquet')\\n\",\n    \"         .option('cloudFiles.schemaLocation', schema_location)\\n\",\n    \"         .option('cloudFiles.schemaEvolutionMode', 'addNewColumns')\\n\",\n    \"         .option('checkpointLocation', checkpoint_location)\\n\",\n    \"         .load(bronze_path)\\n\",\n    \")\\n\",\n    \"from pyspark.sql.functions import upper, col\\n\",\n    \"df_user = df_user.withColumn('username', upper(col('username')))\\n\",\n    \"# Drop rescued data if exists (autoloader may create _rescued_data)\\n\",\n    \"if '_rescued_data' in df_user.columns:\\n\",\n    \"    df_user = df_user.drop('_rescued_data')\\n\",\n    \"# Write stream to Delta (append)\\n\",\n    \"silver_path = f\\\"{base_silver}/dim_user/delta\\\"\\n\",\n    \"(df_user.writeStream\\n\",\n    \"       .format('delta')\\n\",\n    \"       .option('checkpointLocation', checkpoint_location + '/write')\\n\",\n    \"       .outputMode('append')\\n\",\n    \"       .start(silver_path))\\n\"\n   ]\n  }\n ],\n \"metadata\": {},\n \"nbformat\": 4,\n \"nbformat_minor\": 5\n}"